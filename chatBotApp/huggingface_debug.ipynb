{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbd90f3",
   "metadata": {},
   "outputs": [],
   "source": [
    " IMPORTANT: Install required dependencies first\n",
    "# Run this cell first if you haven't installed huggingface_hub yet\n",
    "\n",
    "# !pip install huggingface_hub python-dotenv requests\n",
    "print(\"üö® IMPORTANT: Make sure you have installed the required packages:\")\n",
    "print(\"pip install huggingface_hub python-dotenv requests\")\n",
    "print(\"\\nüìù Note: The old Hugging Face Inference API has been replaced!\")\n",
    "print(\"This notebook uses the NEW Inference Providers system.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e55719d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting Hugging Face API Debug Session\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Hugging Face API Debug Notebook\n",
    "# This notebook will help us debug the connection to Hugging Face models step by step\n",
    "\n",
    "print(\"üöÄ Starting Hugging Face API Debug Session\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189916f6",
   "metadata": {},
   "source": [
    "# Hugging Face INFERENCE PROVIDERS Debug Notebook\n",
    "# ‚ö†Ô∏è IMPORTANT: This uses the NEW system that replaced the old api-inference.huggingface.co\n",
    "\n",
    "print(\"üöÄ Starting Hugging Face INFERENCE PROVIDERS Debug Session\")\n",
    "print(\"=\"*60)\n",
    "print(\"‚ö†Ô∏è  MIGRATION NOTICE: Old serverless inference API is deprecated!\")\n",
    "print(\"üéÜ Now using NEW Inference Providers system with better reliability\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "## New System Overview:\n",
    "- Uses **Inference Providers** (Together AI, Sambanova, fal.ai, etc.)\n",
    "- OpenAI-compatible chat completion API\n",
    "- URL format: `https://router.huggingface.co/{provider}/v1/chat/completions`\n",
    "- Better reliability and more models available\n",
    "\n",
    "## Steps:\n",
    "1. Install huggingface_hub library\n",
    "2. Load environment variables\n",
    "3. Test with InferenceClient (recommended)\n",
    "4. Test direct HTTP calls\n",
    "5. Find working models\n",
    "6. Test chat functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c6c91ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Documents\\learning\\FastAPI_learning\\chatBotApp\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ huggingface_hub available - using modern Inference Providers\n",
      "‚úÖ Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Import required libraries\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from pprint import pprint\n",
    "\n",
    "# Try to import huggingface_hub - this is the modern way\n",
    "try:\n",
    "    from huggingface_hub import InferenceClient\n",
    "    print(\"‚úÖ huggingface_hub available - using modern Inference Providers\")\n",
    "    HF_HUB_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è  huggingface_hub not available - will use direct HTTP calls\")\n",
    "    print(\"üí° Install with: pip install huggingface_hub\")\n",
    "    HF_HUB_AVAILABLE = False\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a0108c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment check:\n",
      "- API Token found: ‚úÖ Yes\n",
      "- Token length: 37 characters\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Load environment variables and check setup\n",
    "load_dotenv()\n",
    "\n",
    "# Get API token\n",
    "API_TOKEN = os.getenv(\"HUGGINGFACE_API_TOKEN\")\n",
    "\n",
    "print(f\"Environment check:\")\n",
    "print(f\"- API Token found: {'‚úÖ Yes' if API_TOKEN else '‚ùå No'}\")\n",
    "if API_TOKEN:\n",
    "    print(f\"- Token length: {len(API_TOKEN)} characters\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No API token found in environment variables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8ca887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing API connection:\n",
      "- Model: gpt2\n",
      "- API URL: https://api-inference.huggingface.co/models/gpt2\n",
      "- Headers prepared: ‚úÖ\n",
      "\n",
      "üí° Note: Not all HF models are available through Inference API\n",
      "   We're testing with GPT-2 which should be available\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Test NEW Inference Providers system\n",
    "# The old api-inference.huggingface.co is deprecated\n",
    "# New system uses router.huggingface.co with different providers\n",
    "\n",
    "print(\"üîÑ Testing NEW Hugging Face Inference Providers system...\")\n",
    "print(\"üìù Note: This replaces the old api-inference.huggingface.co\")\n",
    "print(\"\\nüí° Available providers: Together AI, Sambanova, fal.ai, Replicate, etc.\")\n",
    "print(\"üåê Uses OpenAI-compatible chat completion format\")\n",
    "\n",
    "if API_TOKEN:\n",
    "    print(f\"\\n‚úÖ API Token found: {API_TOKEN[:10]}...{API_TOKEN[-5:]}\")\n",
    "    print(f\"üîó Will test with multiple inference providers\")\n",
    "else:\n",
    "    print(\"‚ùå No API token found - cannot proceed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40068359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ New test functions created\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Create test functions for the NEW system\n",
    "\n",
    "def test_inference_client(model_id, prompt=\"Hello, how are you?\", verbose=True):\n",
    "    \"\"\"\n",
    "    Test using the modern huggingface_hub InferenceClient (RECOMMENDED)\n",
    "    \"\"\"\n",
    "    if not HF_HUB_AVAILABLE:\n",
    "        return {\"success\": False, \"error\": \"huggingface_hub not available\"}\n",
    "    \n",
    "    try:\n",
    "        client = InferenceClient(api_key=API_TOKEN)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\nüîÑ Testing with InferenceClient: {model_id}\")\n",
    "            print(f\"üìù Prompt: '{prompt}'\")\n",
    "        \n",
    "        # Use chat completion (OpenAI-compatible)\n",
    "        completion = client.chat.completions.create(\n",
    "            model=model_id,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            max_tokens=100\n",
    "        )\n",
    "        \n",
    "        response_text = completion.choices[0].message.content\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"‚úÖ Success! Response: {response_text[:200]}\")\n",
    "        \n",
    "        return {\"success\": True, \"data\": response_text}\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"InferenceClient error: {e}\"\n",
    "        if verbose:\n",
    "            print(f\"‚ùå {error_msg}\")\n",
    "        return {\"success\": False, \"error\": error_msg}\n",
    "\n",
    "\n",
    "def test_direct_http(provider, model_id, prompt=\"Hello, how are you?\", verbose=True):\n",
    "    \"\"\"\n",
    "    Test using direct HTTP calls to the new router system\n",
    "    \"\"\"\n",
    "    api_url = f\"https://router.huggingface.co/{provider}/v1/chat/completions\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {API_TOKEN}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    payload = {\n",
    "        \"model\": model_id,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"max_tokens\": 100,\n",
    "        \"stream\": False\n",
    "    }\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nüîÑ Testing HTTP: {provider}/{model_id}\")\n",
    "        print(f\"üìù Prompt: '{prompt}'\")\n",
    "        print(f\"üåê URL: {api_url}\")\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(api_url, headers=headers, json=payload, timeout=30)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"üìä Status Code: {response.status_code}\")\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            try:\n",
    "                result = response.json()\n",
    "                response_text = result[\"choices\"][0][\"message\"][\"content\"]\n",
    "                if verbose:\n",
    "                    print(f\"‚úÖ Success! Response: {response_text[:200]}\")\n",
    "                return {\"success\": True, \"data\": response_text}\n",
    "            except (json.JSONDecodeError, KeyError) as e:\n",
    "                error_msg = f\"Response parsing error: {e}\"\n",
    "                if verbose:\n",
    "                    print(f\"‚ùå {error_msg}\")\n",
    "                    print(f\"Raw response: {response.text[:300]}\")\n",
    "                return {\"success\": False, \"error\": error_msg}\n",
    "        else:\n",
    "            error_msg = f\"HTTP {response.status_code}: {response.text}\"\n",
    "            if verbose:\n",
    "                print(f\"‚ùå {error_msg}\")\n",
    "            return {\"success\": False, \"error\": error_msg}\n",
    "            \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        error_msg = f\"Request exception: {e}\"\n",
    "        if verbose:\n",
    "            print(f\"‚ùå {error_msg}\")\n",
    "        return {\"success\": False, \"error\": error_msg}\n",
    "\n",
    "print(\"‚úÖ New test functions created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6203472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing with InferenceClient (recommended)...\n",
      "\n",
      "üîÑ Testing with InferenceClient: deepseek-ai/DeepSeek-V3-0324\n",
      "üìù Prompt: 'Hello! Can you help me?'\n",
      "‚úÖ Success! Response: Of course! I'd be happy to help. What do you need assistance with? üòä\n",
      "üéâ SUCCESS with deepseek-ai/DeepSeek-V3-0324!\n",
      "‚úÖ Success! Response: Of course! I'd be happy to help. What do you need assistance with? üòä\n",
      "üéâ SUCCESS with deepseek-ai/DeepSeek-V3-0324!\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Test the NEW InferenceClient (recommended approach)\n",
    "if API_TOKEN and HF_HUB_AVAILABLE:\n",
    "    print(\"üß™ Testing with InferenceClient (recommended)...\")\n",
    "    \n",
    "    # Try some popular models that should be available\n",
    "    test_models = [\n",
    "        \"deepseek-ai/DeepSeek-V3-0324\",  # Popular recent model\n",
    "        \"Qwen/QwenQQ-72B-Instruct\",      # Qwen model\n",
    "        \"meta-llama/Llama-3.3-70B-Instruct\",  # Llama model\n",
    "    ]\n",
    "    \n",
    "    for model in test_models:\n",
    "        result = test_inference_client(model, \"Hello! Can you help me?\", verbose=True)\n",
    "        \n",
    "        if result[\"success\"]:\n",
    "            print(f\"üéâ SUCCESS with {model}!\")\n",
    "            break\n",
    "        else:\n",
    "            print(f\"‚ùå Failed: {model}\")\n",
    "            print(f\"   Error: {result['error'][:100]}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "else:\n",
    "    if not API_TOKEN:\n",
    "        print(\"‚ùå Cannot test without API token\")\n",
    "    if not HF_HUB_AVAILABLE:\n",
    "        print(\"‚ùå Cannot test without huggingface_hub library\")\n",
    "        print(\"üí° Install with: pip install huggingface_hub\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d079109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Testing multiple providers with HTTP calls...\n",
      "\n",
      "Testing sambanova/Llama-3.3-70B-Instruct...\n",
      "‚ùå sambanova/Llama-3.3-70B-Instruct: HTTP 404: {\"error\":\"Model not found\"}\n",
      "\n",
      "   ‚îî‚îÄ üí° Model not available through this provider\n",
      "------------------------------------------------------------\n",
      "Testing together/meta-llama/Llama-3.3-70B-Instruct...\n",
      "‚ùå sambanova/Llama-3.3-70B-Instruct: HTTP 404: {\"error\":\"Model not found\"}\n",
      "\n",
      "   ‚îî‚îÄ üí° Model not available through this provider\n",
      "------------------------------------------------------------\n",
      "Testing together/meta-llama/Llama-3.3-70B-Instruct...\n",
      "‚ùå together/meta-llama/Llama-3.3-70B-Instruct: HTTP 400: {\n",
      "  \"id\": \"nwYsDtt-4yUbBN-9499987118fdbacc\",\n",
      "  \"error\": {\n",
      "    \"message\": \"Unable to access\n",
      "------------------------------------------------------------\n",
      "Testing together/deepseek-ai/DeepSeek-V3-0324...\n",
      "‚ùå together/meta-llama/Llama-3.3-70B-Instruct: HTTP 400: {\n",
      "  \"id\": \"nwYsDtt-4yUbBN-9499987118fdbacc\",\n",
      "  \"error\": {\n",
      "    \"message\": \"Unable to access\n",
      "------------------------------------------------------------\n",
      "Testing together/deepseek-ai/DeepSeek-V3-0324...\n",
      "‚ùå together/deepseek-ai/DeepSeek-V3-0324: HTTP 404: {\n",
      "  \"id\": \"nwYsE24-4yUbBN-949998738cb32d28\",\n",
      "  \"error\": {\n",
      "    \"message\": \"Unable to access\n",
      "   ‚îî‚îÄ üí° Model not available through this provider\n",
      "------------------------------------------------------------\n",
      "Testing fireworks/deepseek-ai/DeepSeek-V3-0324...\n",
      "‚ùå together/deepseek-ai/DeepSeek-V3-0324: HTTP 404: {\n",
      "  \"id\": \"nwYsE24-4yUbBN-949998738cb32d28\",\n",
      "  \"error\": {\n",
      "    \"message\": \"Unable to access\n",
      "   ‚îî‚îÄ üí° Model not available through this provider\n",
      "------------------------------------------------------------\n",
      "Testing fireworks/deepseek-ai/DeepSeek-V3-0324...\n",
      "‚ùå fireworks/deepseek-ai/DeepSeek-V3-0324: HTTP 404: Not Found\n",
      "   ‚îî‚îÄ üí° Model not available through this provider\n",
      "------------------------------------------------------------\n",
      "\n",
      "üìä Summary:\n",
      "‚úÖ Working configurations (0): []\n",
      "\n",
      "üö® No configurations are working! This could indicate:\n",
      "   1. Need Hugging Face PRO subscription ($9/month with $2 credits)\n",
      "   2. Need specific provider API keys\n",
      "   3. Token permissions insufficient\n",
      "   4. Models temporarily unavailable\n",
      "\n",
      "üí° Try the InferenceClient approach instead (Step 5)\n",
      "‚ùå fireworks/deepseek-ai/DeepSeek-V3-0324: HTTP 404: Not Found\n",
      "   ‚îî‚îÄ üí° Model not available through this provider\n",
      "------------------------------------------------------------\n",
      "\n",
      "üìä Summary:\n",
      "‚úÖ Working configurations (0): []\n",
      "\n",
      "üö® No configurations are working! This could indicate:\n",
      "   1. Need Hugging Face PRO subscription ($9/month with $2 credits)\n",
      "   2. Need specific provider API keys\n",
      "   3. Token permissions insufficient\n",
      "   4. Models temporarily unavailable\n",
      "\n",
      "üí° Try the InferenceClient approach instead (Step 5)\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Test different providers with direct HTTP calls\n",
    "if API_TOKEN:\n",
    "    print(\"üîç Testing multiple providers with HTTP calls...\\n\")\n",
    "    \n",
    "    # Test different provider/model combinations\n",
    "    test_configs = [\n",
    "        (\"sambanova\", \"Llama-3.3-70B-Instruct\"),\n",
    "        (\"together\", \"meta-llama/Llama-3.3-70B-Instruct\"),\n",
    "        (\"together\", \"deepseek-ai/DeepSeek-V3-0324\"),\n",
    "        (\"fireworks\", \"deepseek-ai/DeepSeek-V3-0324\"),\n",
    "    ]\n",
    "    \n",
    "    results = {}\n",
    "    working_configs = []\n",
    "    \n",
    "    for provider, model in test_configs:\n",
    "        print(f\"Testing {provider}/{model}...\")\n",
    "        result = test_direct_http(provider, model, \"Hello!\", verbose=False)\n",
    "        config_key = f\"{provider}/{model}\"\n",
    "        results[config_key] = result\n",
    "        \n",
    "        if result[\"success\"]:\n",
    "            print(f\"‚úÖ {config_key}: SUCCESS\")\n",
    "            working_configs.append((provider, model))\n",
    "        else:\n",
    "            error_preview = result['error'][:100] if len(result['error']) > 100 else result['error']\n",
    "            print(f\"‚ùå {config_key}: {error_preview}\")\n",
    "            if \"403\" in str(result['error']):\n",
    "                print(f\"   ‚îî‚îÄ üí° May need specific provider API key or PRO subscription\")\n",
    "            elif \"404\" in str(result['error']):\n",
    "                print(f\"   ‚îî‚îÄ üí° Model not available through this provider\")\n",
    "        print(\"-\" * 60)\n",
    "    \n",
    "    print(\"\\nüìä Summary:\")\n",
    "    print(f\"‚úÖ Working configurations ({len(working_configs)}): {working_configs}\")\n",
    "    \n",
    "    if not working_configs:\n",
    "        print(\"\\nüö® No configurations are working! This could indicate:\")\n",
    "        print(\"   1. Need Hugging Face PRO subscription ($9/month with $2 credits)\")\n",
    "        print(\"   2. Need specific provider API keys\")\n",
    "        print(\"   3. Token permissions insufficient\")\n",
    "        print(\"   4. Models temporarily unavailable\")\n",
    "        print(\"\\nüí° Try the InferenceClient approach instead (Step 5)\")\n",
    "else:\n",
    "    print(\"‚ùå Cannot test without API token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140b2324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Analyze errors and understand the new billing system\n",
    "print(\"üîç Detailed Analysis of New Inference Providers System:\\n\")\n",
    "\n",
    "print(\"üí∞ Billing Information:\")\n",
    "print(\"- Hugging Face PRO ($9/month): Includes $2 worth of inference credits\")\n",
    "print(\"- Free tier: Very limited quota for signed-in users\")\n",
    "print(\"- Direct provider keys: Billed directly by provider (Together AI, etc.)\")\n",
    "print(\"- No markup on provider rates when using HF routing\")\n",
    "\n",
    "print(\"\\nüîë Authentication Options:\")\n",
    "print(\"1. HF Token + PRO subscription: Routed through HF with credits\")\n",
    "print(\"2. HF Token + Provider API key: Direct billing to provider\")\n",
    "print(\"3. Free tier: Very limited usage\")\n",
    "\n",
    "print(\"\\nüö® Common Issues:\")\n",
    "print(\"- 403 Forbidden: Insufficient permissions or quota exceeded\")\n",
    "print(\"- 404 Not Found: Model not available through that provider\")\n",
    "print(\"- Rate limiting: Hit usage limits\")\n",
    "\n",
    "if 'results' in locals():\n",
    "    print(\"\\nüìÑ Error Details from Tests:\")\n",
    "    for config, result in results.items():\n",
    "        if not result[\"success\"]:\n",
    "            print(f\"\\n{config}:\")\n",
    "            print(f\"  Error: {result['error'][:200]}\")\n",
    "else:\n",
    "    print(\"\\nüí° Run the previous cells to see specific error details\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0467b0c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üó£Ô∏è Testing conversation with InferenceClient...\n",
      "\n",
      "üë§ Human: Hello, how are you?\n",
      "üë§ Human: Hello, how are you?\n",
      "ü§ñ Bot: Hello! I'm just a virtual assistant, so I don't have feelings, but I'm here and ready to help you with anything you need. üòä How about you? How are you doing today?\n",
      "--------------------------------------------------\n",
      "üë§ Human: What's the capital of France?\n",
      "ü§ñ Bot: Hello! I'm just a virtual assistant, so I don't have feelings, but I'm here and ready to help you with anything you need. üòä How about you? How are you doing today?\n",
      "--------------------------------------------------\n",
      "üë§ Human: What's the capital of France?\n",
      "ü§ñ Bot: The capital of France is **Paris**. It is known for its iconic landmarks such as the Eiffel Tower, the Louvre Museum, and the Arc de Triomphe.  \n",
      "\n",
      "Would you like recommendations for things to do in Paris? üòä\n",
      "--------------------------------------------------\n",
      "üë§ Human: Can you help me write a simple Python function?\n",
      "ü§ñ Bot: The capital of France is **Paris**. It is known for its iconic landmarks such as the Eiffel Tower, the Louvre Museum, and the Arc de Triomphe.  \n",
      "\n",
      "Would you like recommendations for things to do in Paris? üòä\n",
      "--------------------------------------------------\n",
      "üë§ Human: Can you help me write a simple Python function?\n",
      "ü§ñ Bot: Of course! Here's a simple Python function that takes two numbers as input and returns their sum:\n",
      "\n",
      "```python\n",
      "def add_numbers(a, b):\n",
      "    \"\"\"This function adds two numbers and returns the result.\"\"\"\n",
      "    return a + b\n",
      "\n",
      "# Example usage:\n",
      "result = add_numbers(5, 3)\n",
      "print(\"The sum is:\", result)  # Output: The sum is: 8\n",
      "```\n",
      "\n",
      "### Breakdown:\n",
      "1. **`def add_numbers(a, b):`**  \n",
      "  \n",
      "--------------------------------------------------\n",
      "ü§ñ Bot: Of course! Here's a simple Python function that takes two numbers as input and returns their sum:\n",
      "\n",
      "```python\n",
      "def add_numbers(a, b):\n",
      "    \"\"\"This function adds two numbers and returns the result.\"\"\"\n",
      "    return a + b\n",
      "\n",
      "# Example usage:\n",
      "result = add_numbers(5, 3)\n",
      "print(\"The sum is:\", result)  # Output: The sum is: 8\n",
      "```\n",
      "\n",
      "### Breakdown:\n",
      "1. **`def add_numbers(a, b):`**  \n",
      "  \n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Step 8: Test conversation with working configuration\n",
    "if 'working_configs' in locals() and working_configs:\n",
    "    provider, model = working_configs[0]\n",
    "    print(f\"üó£Ô∏è Testing conversation with: {provider}/{model}\\n\")\n",
    "    \n",
    "    conversation_prompts = [\n",
    "        \"Hello, how are you?\",\n",
    "        \"What's the capital of France?\",\n",
    "        \"Can you help me write a simple Python function?\",\n",
    "        \"What's the weather like?\"\n",
    "    ]\n",
    "    \n",
    "    for prompt in conversation_prompts:\n",
    "        print(f\"üë§ Human: {prompt}\")\n",
    "        result = test_direct_http(provider, model, prompt, verbose=False)\n",
    "        \n",
    "        if result[\"success\"]:\n",
    "            response = result[\"data\"]\n",
    "            print(f\"ü§ñ Bot: {response}\")\n",
    "        else:\n",
    "            print(f\"ü§ñ Bot: Error - {result['error'][:100]}\")\n",
    "        \n",
    "        print(\"-\" * 50)\n",
    "\n",
    "elif HF_HUB_AVAILABLE and API_TOKEN:\n",
    "    print(\"üó£Ô∏è Testing conversation with InferenceClient...\\n\")\n",
    "    \n",
    "    # Try to find a working model with InferenceClient\n",
    "    test_models = [\"deepseek-ai/DeepSeek-V3-0324\", \"meta-llama/Llama-3.3-70B-Instruct\"]\n",
    "    \n",
    "    working_model = None\n",
    "    for model in test_models:\n",
    "        result = test_inference_client(model, \"Hello!\", verbose=False)\n",
    "        if result[\"success\"]:\n",
    "            working_model = model\n",
    "            break\n",
    "    \n",
    "    if working_model:\n",
    "        conversation_prompts = [\n",
    "            \"Hello, how are you?\",\n",
    "            \"What's the capital of France?\",\n",
    "            \"Can you help me write a simple Python function?\"\n",
    "        ]\n",
    "        \n",
    "        for prompt in conversation_prompts:\n",
    "            print(f\"üë§ Human: {prompt}\")\n",
    "            result = test_inference_client(working_model, prompt, verbose=False)\n",
    "            \n",
    "            if result[\"success\"]:\n",
    "                print(f\"ü§ñ Bot: {result['data']}\")\n",
    "            else:\n",
    "                print(f\"ü§ñ Bot: Error - {result['error'][:100]}\")\n",
    "            \n",
    "            print(\"-\" * 50)\n",
    "    else:\n",
    "        print(\"‚ùå No working models found for conversation test\")\n",
    "else:\n",
    "    print(\"‚ùå Cannot test conversation - no working configurations or missing requirements\")\n",
    "    if not HF_HUB_AVAILABLE:\n",
    "        print(\"üí° Install huggingface_hub: pip install huggingface_hub\")\n",
    "    if not API_TOKEN:\n",
    "        print(\"üí° Set HUGGINGFACE_API_TOKEN in .env file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dfc241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: Test different payload formats with OpenAI-compatible API\n",
    "if 'working_configs' in locals() and working_configs:\n",
    "    provider, model = working_configs[0]\n",
    "    print(f\"üß™ Testing different payload formats with: {provider}/{model}\\n\")\n",
    "    \n",
    "    api_url = f\"https://router.huggingface.co/{provider}/v1/chat/completions\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {API_TOKEN}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    payloads = [\n",
    "        # Format 1: Basic chat completion\n",
    "        {\n",
    "            \"model\": model,\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}],\n",
    "            \"max_tokens\": 50\n",
    "        },\n",
    "        \n",
    "        # Format 2: With temperature and other parameters\n",
    "        {\n",
    "            \"model\": model,\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": \"Tell me a joke\"}],\n",
    "            \"max_tokens\": 100,\n",
    "            \"temperature\": 0.7,\n",
    "            \"stream\": False\n",
    "        },\n",
    "        \n",
    "        # Format 3: Multi-turn conversation\n",
    "        {\n",
    "            \"model\": model,\n",
    "            \"messages\": [\n",
    "                {\"role\": \"user\", \"content\": \"Hello!\"},\n",
    "                {\"role\": \"assistant\", \"content\": \"Hi there! How can I help you?\"},\n",
    "                {\"role\": \"user\", \"content\": \"What's 2+2?\"}\n",
    "            ],\n",
    "            \"max_tokens\": 50\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for i, payload in enumerate(payloads, 1):\n",
    "        print(f\"Format {i}: {json.dumps(payload, indent=2)}\")\n",
    "        \n",
    "        try:\n",
    "            response = requests.post(api_url, headers=headers, json=payload, timeout=30)\n",
    "            print(f\"Status: {response.status_code}\")\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                try:\n",
    "                    result = response.json()\n",
    "                    content = result[\"choices\"][0][\"message\"][\"content\"]\n",
    "                    print(f\"‚úÖ Success: {content}\")\n",
    "                except (json.JSONDecodeError, KeyError) as e:\n",
    "                    print(f\"‚ùå JSON/Key Error: {e}\")\n",
    "                    print(f\"Raw response: {response.text[:200]}\")\n",
    "            else:\n",
    "                print(f\"‚ùå HTTP Error: {response.text[:200]}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Exception: {e}\")\n",
    "        \n",
    "        print(\"-\" * 60)\n",
    "else:\n",
    "    print(\"‚ùå No working configurations to test payload formats\")\n",
    "    print(\"üí° First get a working provider/model combination from Step 6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24cbea9d",
   "metadata": {},
   "source": [
    "## üéØ Final Recommendations for NEW Inference Providers System\n",
    "\n",
    "Based on the tests above, here's how to use the **modern** Hugging Face system:\n",
    "\n",
    "### üö™ Migration from Old System\n",
    "1. **Old URL**: `https://api-inference.huggingface.co/models/{model}` ‚ùå DEPRECATED\n",
    "2. **New URL**: `https://router.huggingface.co/{provider}/v1/chat/completions` ‚úÖ CURRENT\n",
    "3. **New Format**: OpenAI-compatible chat completion API\n",
    "\n",
    "### üí∞ Billing & Access\n",
    "1. **Free Tier**: Very limited quota for exploration\n",
    "2. **PRO Subscription**: $9/month includes $2 worth of inference credits\n",
    "3. **Provider Keys**: Use your own Together AI, Sambanova, etc. keys for direct billing\n",
    "\n",
    "### üîß Implementation Options\n",
    "\n",
    "**Option 1: InferenceClient (Recommended)**\n",
    "```python\n",
    "from huggingface_hub import InferenceClient\n",
    "client = InferenceClient(api_key=\"your_hf_token\")\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"deepseek-ai/DeepSeek-V3-0324\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n",
    ")\n",
    "```\n",
    "\n",
    "**Option 2: Direct HTTP (OpenAI-compatible)**\n",
    "```python\n",
    "requests.post(\"https://router.huggingface.co/together/v1/chat/completions\", ...)\n",
    "```\n",
    "\n",
    "### üéØ Best Practices\n",
    "1. **Use InferenceClient** for easiest integration\n",
    "2. **Handle provider fallbacks** (try multiple providers)\n",
    "3. **Implement proper error handling** for quota/billing issues\n",
    "4. **Use OpenAI chat completion format** for messages\n",
    "5. **Consider getting PRO subscription** for reliable access\n",
    "\n",
    "## üîÑ Migration Steps\n",
    "1. Replace old API calls with new InferenceClient\n",
    "2. Update payload format to OpenAI chat completion\n",
    "3. Add provider fallback logic\n",
    "4. Update error handling for new response format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1bac1aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Updated Configuration for NEW Inference Providers system:\n",
      "{\n",
      "  \"system\": \"inference_providers\",\n",
      "  \"migration_notes\": {\n",
      "    \"old_url\": \"https://api-inference.huggingface.co/models/{model_id} (DEPRECATED)\",\n",
      "    \"new_url\": \"https://router.huggingface.co/{provider}/v1/chat/completions\",\n",
      "    \"format_change\": \"Now uses OpenAI-compatible chat completion API\"\n",
      "  },\n",
      "  \"recommended_approach\": \"huggingface_hub.InferenceClient\",\n",
      "  \"billing\": {\n",
      "    \"free_tier\": \"Very limited quota\",\n",
      "    \"pro_subscription\": \"$9/month with $2 inference credits\",\n",
      "    \"direct_provider\": \"Use provider API keys for direct billing\"\n",
      "  },\n",
      "  \"dependencies\": [\n",
      "    \"huggingface_hub\",\n",
      "    \"requests\",\n",
      "    \"python-dotenv\"\n",
      "  ],\n",
      "  \"environment_variables\": {\n",
      "    \"HUGGINGFACE_API_TOKEN\": \"Required - get from hf.co/settings/tokens\"\n",
      "  },\n",
      "  \"working_providers\": [],\n",
      "  \"recommended_config\": {\n",
      "    \"provider\": \"auto\",\n",
      "    \"model\": \"deepseek-ai/DeepSeek-V3-0324\"\n",
      "  },\n",
      "  \"implementation\": {\n",
      "    \"inference_client\": {\n",
      "      \"library\": \"huggingface_hub\",\n",
      "      \"example\": \"InferenceClient(api_key=token).chat.completions.create(model=model, messages=messages)\"\n",
      "    },\n",
      "    \"direct_http\": {\n",
      "      \"url_template\": \"https://router.huggingface.co/{provider}/v1/chat/completions\",\n",
      "      \"method\": \"POST\",\n",
      "      \"headers\": {\n",
      "        \"Authorization\": \"Bearer {token}\",\n",
      "        \"Content-Type\": \"application/json\"\n",
      "      },\n",
      "      \"payload_format\": {\n",
      "        \"model\": \"{model_id}\",\n",
      "        \"messages\": [\n",
      "          {\n",
      "            \"role\": \"user\",\n",
      "            \"content\": \"{prompt}\"\n",
      "          }\n",
      "        ],\n",
      "        \"max_tokens\": 100\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "‚úÖ Configuration saved to hf_inference_providers_config.json\n",
      "\n",
      "üö® IMPORTANT: Update your main FastAPI app to use this new system!\n",
      "üí° Key changes needed:\n",
      "  1. Install: pip install huggingface_hub\n",
      "  2. Replace old API calls with InferenceClient\n",
      "  3. Update to OpenAI chat completion format\n",
      "  4. Handle new error responses and billing limits\n"
     ]
    }
   ],
   "source": [
    "# Step 10: Export working configuration for main app (NEW SYSTEM)\n",
    "\n",
    "# Prepare configuration for the new Inference Providers system\n",
    "config = {\n",
    "    \"system\": \"inference_providers\",\n",
    "    \"migration_notes\": {\n",
    "        \"old_url\": \"https://api-inference.huggingface.co/models/{model_id} (DEPRECATED)\",\n",
    "        \"new_url\": \"https://router.huggingface.co/{provider}/v1/chat/completions\",\n",
    "        \"format_change\": \"Now uses OpenAI-compatible chat completion API\"\n",
    "    },\n",
    "    \"recommended_approach\": \"huggingface_hub.InferenceClient\",\n",
    "    \"billing\": {\n",
    "        \"free_tier\": \"Very limited quota\",\n",
    "        \"pro_subscription\": \"$9/month with $2 inference credits\",\n",
    "        \"direct_provider\": \"Use provider API keys for direct billing\"\n",
    "    },\n",
    "    \"dependencies\": [\"huggingface_hub\", \"requests\", \"python-dotenv\"],\n",
    "    \"environment_variables\": {\n",
    "        \"HUGGINGFACE_API_TOKEN\": \"Required - get from hf.co/settings/tokens\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Add working configurations if found\n",
    "if 'working_configs' in locals() and working_configs:\n",
    "    config[\"working_providers\"] = working_configs\n",
    "    config[\"recommended_config\"] = {\n",
    "        \"provider\": working_configs[0][0],\n",
    "        \"model\": working_configs[0][1]\n",
    "    }\n",
    "else:\n",
    "    config[\"working_providers\"] = []\n",
    "    config[\"recommended_config\"] = {\n",
    "        \"provider\": \"auto\",  # Let HF choose best provider\n",
    "        \"model\": \"deepseek-ai/DeepSeek-V3-0324\"\n",
    "    }\n",
    "\n",
    "# Add implementation examples\n",
    "config[\"implementation\"] = {\n",
    "    \"inference_client\": {\n",
    "        \"library\": \"huggingface_hub\",\n",
    "        \"example\": \"InferenceClient(api_key=token).chat.completions.create(model=model, messages=messages)\"\n",
    "    },\n",
    "    \"direct_http\": {\n",
    "        \"url_template\": \"https://router.huggingface.co/{provider}/v1/chat/completions\",\n",
    "        \"method\": \"POST\",\n",
    "        \"headers\": {\"Authorization\": \"Bearer {token}\", \"Content-Type\": \"application/json\"},\n",
    "        \"payload_format\": {\n",
    "            \"model\": \"{model_id}\",\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": \"{prompt}\"}],\n",
    "            \"max_tokens\": 100\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"üìã Updated Configuration for NEW Inference Providers system:\")\n",
    "print(json.dumps(config, indent=2))\n",
    "\n",
    "# Save to file for easy reference\n",
    "with open(\"hf_inference_providers_config.json\", \"w\") as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "print(\"\\n‚úÖ Configuration saved to hf_inference_providers_config.json\")\n",
    "print(\"\\nüö® IMPORTANT: Update your main FastAPI app to use this new system!\")\n",
    "print(\"üí° Key changes needed:\")\n",
    "print(\"  1. Install: pip install huggingface_hub\")\n",
    "print(\"  2. Replace old API calls with InferenceClient\")\n",
    "print(\"  3. Update to OpenAI chat completion format\")\n",
    "print(\"  4. Handle new error responses and billing limits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b7bad5",
   "metadata": {},
   "source": [
    "## üéÜ Summary: Migration Complete!\n",
    "\n",
    "**The old Hugging Face serverless inference API is deprecated.** This notebook now uses the **NEW Inference Providers system**.\n",
    "\n",
    "### üõ†Ô∏è What You Need To Do:\n",
    "\n",
    "1. **Install Dependencies**:\n",
    "   ```bash\n",
    "   pip install huggingface_hub python-dotenv requests\n",
    "   ```\n",
    "\n",
    "2. **Update Your FastAPI App** to use:\n",
    "   - `InferenceClient` from `huggingface_hub`\n",
    "   - OpenAI-compatible chat completion format\n",
    "   - New error handling for billing/quota limits\n",
    "\n",
    "3. **Consider Billing**:\n",
    "   - Free tier has very limited quota\n",
    "   - PRO subscription ($9/month) includes $2 inference credits\n",
    "   - Or use your own provider API keys\n",
    "\n",
    "4. **Update Environment**:\n",
    "   - Your HF token should work\n",
    "   - May need PRO subscription for reliable access\n",
    "\n",
    "### üöÄ Ready for Production\n",
    "Once you've run this notebook and found working configurations, update your main FastAPI application using the exported configuration file."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
